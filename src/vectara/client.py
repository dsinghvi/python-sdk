# This file was auto-generated by Fern from our API Definition.

import json
import os
import typing
from json.decoder import JSONDecodeError

import httpx

from . import core
from .api_keys.client import ApiKeysClient, AsyncApiKeysClient
from .application_clients.client import ApplicationClientsClient, AsyncApplicationClientsClient
from .auth.client import AsyncAuthClient, AuthClient
from .chats.client import AsyncChatsClient, ChatsClient
from .core.api_error import ApiError
from .core.client_wrapper import AsyncClientWrapper, SyncClientWrapper
from .core.jsonable_encoder import jsonable_encoder
from .core.oauth_token_provider import OAuthTokenProvider
from .core.pydantic_utilities import pydantic_v1
from .core.request_options import RequestOptions
from .corpus.client import AsyncCorpusClient, CorpusClient
from .documents.client import AsyncDocumentsClient, DocumentsClient
from .encoders.client import AsyncEncodersClient, EncodersClient
from .environment import VectaraEnvironment
from .errors.bad_request_error import BadRequestError
from .errors.forbidden_error import ForbiddenError
from .errors.not_found_error import NotFoundError
from .jobs.client import AsyncJobsClient, JobsClient
from .llms.client import AsyncLlmsClient, LlmsClient
from .rerankers.client import AsyncRerankersClient, RerankersClient
from .types.bad_request_error_body import BadRequestErrorBody
from .types.corpus_key import CorpusKey
from .types.document import Document
from .types.error import Error
from .types.generation_parameters import GenerationParameters
from .types.not_found_error_body import NotFoundErrorBody
from .types.query_full_response import QueryFullResponse
from .types.query_streamed_response import QueryStreamedResponse
from .types.search_corpora_parameters import SearchCorporaParameters
from .users.client import AsyncUsersClient, UsersClient

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class Vectara:
    """
    Use this class to access the different functions within the SDK. You can instantiate any number of clients with different configuration that will propagate to these functions.

    Parameters
    ----------
    base_url : typing.Optional[str]
        The base url to use for requests from the client.

    environment : VectaraEnvironment
        The environment to use for requests from the client. from .environment import VectaraEnvironment



        Defaults to VectaraEnvironment.DEFAULT



    api_key : typing.Optional[str]
    client_id : typing.Optional[str]
    client_secret : typing.Optional[str]
    _token_getter_override : typing.Optional[typing.Callable[[], str]]
    timeout : typing.Optional[float]
        The timeout to be used, in seconds, for requests. By default the timeout is 60 seconds, unless a custom httpx client is used, in which case this default is not enforced.

    follow_redirects : typing.Optional[bool]
        Whether the default httpx client follows redirects or not, this is irrelevant if a custom httpx client is passed in.

    httpx_client : typing.Optional[httpx.Client]
        The httpx client to use for making requests, a preconfigured client is used by default, however this is useful should you want to pass in any custom httpx configuration.

    Examples
    --------
    from vectara.client import Vectara

    client = Vectara(
        api_key="YOUR_API_KEY",
    )
    """

    def __init__(
        self,
        *,
        base_url: typing.Optional[str] = None,
        environment: VectaraEnvironment = VectaraEnvironment.DEFAULT,
        api_key: typing.Optional[str] = os.getenv("VECTARA_API_KEY"),
        client_id: typing.Optional[str] = os.getenv("VECTARA_CLIENT_ID"),
        client_secret: typing.Optional[str] = os.getenv("VECTARA_CLIENT_SECRET"),
        _token_getter_override: typing.Optional[typing.Callable[[], str]] = None,
        timeout: typing.Optional[float] = None,
        follow_redirects: typing.Optional[bool] = True,
        httpx_client: typing.Optional[httpx.Client] = None,
    ):
        _defaulted_timeout = timeout if timeout is not None else 60 if httpx_client is None else None
        if client_id is None:
            raise ApiError(
                body="The client must be instantiated be either passing in client_id or setting VECTARA_CLIENT_ID"
            )
        if client_secret is None:
            raise ApiError(
                body="The client must be instantiated be either passing in client_secret or setting VECTARA_CLIENT_SECRET"
            )
        oauth_token_provider = OAuthTokenProvider(
            client_id=client_id,
            client_secret=client_secret,
            client_wrapper=SyncClientWrapper(
                base_url=_get_base_url(base_url=base_url, environment=environment),
                api_key=api_key,
                httpx_client=httpx.Client(timeout=_defaulted_timeout, follow_redirects=follow_redirects)
                if follow_redirects is not None
                else httpx.Client(timeout=_defaulted_timeout),
                timeout=_defaulted_timeout,
            ),
        )
        self._client_wrapper = SyncClientWrapper(
            base_url=_get_base_url(base_url=base_url, environment=environment),
            api_key=api_key,
            token=_token_getter_override if _token_getter_override is not None else oauth_token_provider.get_token,
            httpx_client=httpx_client
            if httpx_client is not None
            else httpx.Client(timeout=_defaulted_timeout, follow_redirects=follow_redirects)
            if follow_redirects is not None
            else httpx.Client(timeout=_defaulted_timeout),
            timeout=_defaulted_timeout,
        )
        self.api_keys = ApiKeysClient(client_wrapper=self._client_wrapper)
        self.application_clients = ApplicationClientsClient(client_wrapper=self._client_wrapper)
        self.auth = AuthClient(client_wrapper=self._client_wrapper)
        self.chats = ChatsClient(client_wrapper=self._client_wrapper)
        self.corpus = CorpusClient(client_wrapper=self._client_wrapper)
        self.documents = DocumentsClient(client_wrapper=self._client_wrapper)
        self.encoders = EncodersClient(client_wrapper=self._client_wrapper)
        self.jobs = JobsClient(client_wrapper=self._client_wrapper)
        self.llms = LlmsClient(client_wrapper=self._client_wrapper)
        self.rerankers = RerankersClient(client_wrapper=self._client_wrapper)
        self.users = UsersClient(client_wrapper=self._client_wrapper)

    @typing.overload
    def query(
        self,
        *,
        stream_response: typing.Literal[False] = False,
        query: str,
        search: SearchCorporaParameters,
        generation: typing.Optional[GenerationParameters] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> QueryFullResponse:
        """
        Perform a multi-purpose query that can retrieve relevant information from one or more corpora and generate a response using RAG.

        Parameters
        ----------
        stream_response: bool
            Whether or not to stream the response

        query : str
            The query to receive an answer on.

        search : SearchCorporaParameters

        generation : typing.Optional[GenerationParameters]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Yields
        ------
        QueryFullResponse


        Examples
        --------
        from vectara import (
            CitationParameters,
            ContextConfiguration,
            GenerationParameters,
            KeyedSearchCorpus,
            ModelParameters,
            SearchCorporaParameters,
            SearchReranker_CustomerReranker,
        )
        from vectara.client import Vectara

        client = Vectara(
            api_key="YOUR_API_KEY",
        )
        response = client.query(
            query="string",
            search=SearchCorporaParameters(
                corpora=[KeyedSearchCorpus()],
                offset=1,
                limit=1,
                context_configuration=ContextConfiguration(),
                reranker=SearchReranker_CustomerReranker(),
            ),
            generation=GenerationParameters(
                prompt_name="string",
                max_used_search_results=1,
                prompt_text="string",
                max_response_characters=1,
                response_language="auto",
                model_parameters=ModelParameters(
                    max_tokens=1,
                    temperature=1.1,
                    frequency_penalty=1.1,
                    presence_penalty=1.1,
                ),
                citations=CitationParameters(),
                enable_factual_consistency_score=True,
            ),
        )
        """
        ...
    
    @typing.overload
    def query(
        self,
        *,
        stream_response: typing.Literal[True],
        query: str,
        search: SearchCorporaParameters,
        generation: typing.Optional[GenerationParameters] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.Iterator[QueryStreamedResponse]:
        """
        Perform a multi-purpose query that can retrieve relevant information from one or more corpora and generate a response using RAG.

        Parameters
        ----------
        stream_response: bool
            Whether or not to stream the response

        query : str
            The query to receive an answer on.

        search : SearchCorporaParameters

        generation : typing.Optional[GenerationParameters]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Yields
        ------
        typing.Iterator[QueryStreamedResponse]


        Examples
        --------
        from vectara import (
            CitationParameters,
            ContextConfiguration,
            GenerationParameters,
            KeyedSearchCorpus,
            ModelParameters,
            SearchCorporaParameters,
            SearchReranker_CustomerReranker,
        )
        from vectara.client import Vectara

        client = Vectara(
            api_key="YOUR_API_KEY",
        )
        response = client.query(
            query="string",
            search=SearchCorporaParameters(
                corpora=[KeyedSearchCorpus()],
                offset=1,
                limit=1,
                context_configuration=ContextConfiguration(),
                reranker=SearchReranker_CustomerReranker(),
            ),
            generation=GenerationParameters(
                prompt_name="string",
                max_used_search_results=1,
                prompt_text="string",
                max_response_characters=1,
                response_language="auto",
                model_parameters=ModelParameters(
                    max_tokens=1,
                    temperature=1.1,
                    frequency_penalty=1.1,
                    presence_penalty=1.1,
                ),
                citations=CitationParameters(),
                enable_factual_consistency_score=True,
            ),
        )
        for chunk in response:
            yield chunk
        """
        ...

    def query(
        self,
        *,
        stream_response: bool = False,
        query: str,
        search: SearchCorporaParameters,
        generation: typing.Optional[GenerationParameters] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.Union[QueryFullResponse, typing.Iterator[QueryStreamedResponse]]:
        """
        Perform a multi-purpose query that can retrieve relevant information from one or more corpora and generate a response using RAG.

        Parameters
        ----------
        stream_response: bool
            Whether or not to stream the response        

        query : str
            The query to receive an answer on.

        search : SearchCorporaParameters

        generation : typing.Optional[GenerationParameters]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.Union[QueryFullResponse, typing.Iterator[QueryStreamedResponse]]


        Examples
        --------
        from vectara import (
            CitationParameters,
            ContextConfiguration,
            GenerationParameters,
            KeyedSearchCorpus,
            ModelParameters,
            SearchCorporaParameters,
            SearchReranker_CustomerReranker,
        )
        from vectara.client import Vectara

        client = Vectara(
            api_key="YOUR_API_KEY",
        )
        client.query(
            query="Am I allowed to bring pets to work?",
            search=SearchCorporaParameters(
                corpora=[
                    KeyedSearchCorpus(
                        custom_dimensions={},
                        metadata_filter='doc.title = "Adventures of Huckleberry Finn"',
                        lexical_interpolation=0.025,
                        semantics="default",
                        corpus_key="my-corpus",
                    )
                ],
                offset=0,
                limit=10,
                context_configuration=ContextConfiguration(
                    characters_before=30,
                    characters_after=30,
                    sentences_before=3,
                    sentences_after=3,
                    start_tag="<em>",
                    end_tag="</em>",
                ),
                reranker=SearchReranker_CustomerReranker(
                    reranker_id="rnk_272725719",
                ),
            ),
            generation=GenerationParameters(
                prompt_name="vectara-summary-ext-v1.2.0",
                max_used_search_results=5,
                prompt_text='[\n  {"role": "system", "content": "You are a helpful search assistant."},\n  #foreach ($qResult in $vectaraQueryResults)\n    {"role": "user", "content": "Given the $vectaraIdxWord[$foreach.index] search result."},\n    {"role": "assistant", "content": "${qResult.getText()}" },\n  #end\n  {"role": "user", "content": "Generate a summary for the query \'${vectaraQuery}\' based on the above results."}\n]\n',
                max_response_characters=300,
                response_language="auto",
                model_parameters=ModelParameters(
                    max_tokens=0,
                    temperature=0.0,
                    frequency_penalty=0.0,
                    presence_penalty=0.0,
                ),
                citations=CitationParameters(
                    style="none",
                    url_pattern="https://vectara.com/documents/{doc.id}",
                    text_pattern="{doc.title}",
                ),
                enable_factual_consistency_score=True,
            ),
        )
        """
        if stream_response: 
            with self._client_wrapper.httpx_client.stream(
                "v2/query",
                method="POST",
                json={"query": query, "search": search, "generation": generation, "stream_response": True},
                request_options=request_options,
                omit=OMIT,
            ) as _response:
                if 200 <= _response.status_code < 300:
                    for _text in _response.iter_lines():
                        if len(_text) == 0:
                            continue
                        yield pydantic_v1.parse_obj_as(QueryStreamedResponse, json.loads(_text))  # type: ignore
                    return
                _response.read()
                if _response.status_code == 400:
                    raise BadRequestError(pydantic_v1.parse_obj_as(BadRequestErrorBody, _response.json()))  # type: ignore
                if _response.status_code == 403:
                    raise ForbiddenError(pydantic_v1.parse_obj_as(Error, _response.json()))  # type: ignore
                if _response.status_code == 404:
                    raise NotFoundError(pydantic_v1.parse_obj_as(NotFoundErrorBody, _response.json()))  # type: ignore
                try:
                    _response_json = _response.json()
                except JSONDecodeError:
                    raise ApiError(status_code=_response.status_code, body=_response.text)
                raise ApiError(status_code=_response.status_code, body=_response_json)
        else: 
            _response = self._client_wrapper.httpx_client.request(
                "v2/query",
                method="POST",
                json={"query": query, "search": search, "generation": generation, "stream_response": False},
                request_options=request_options,
                omit=OMIT,
            )
            if 200 <= _response.status_code < 300:
                return pydantic_v1.parse_obj_as(QueryFullResponse, _response.json())  # type: ignore
            if _response.status_code == 400:
                raise BadRequestError(pydantic_v1.parse_obj_as(BadRequestErrorBody, _response.json()))  # type: ignore
            if _response.status_code == 403:
                raise ForbiddenError(pydantic_v1.parse_obj_as(Error, _response.json()))  # type: ignore
            if _response.status_code == 404:
                raise NotFoundError(pydantic_v1.parse_obj_as(NotFoundErrorBody, _response.json()))  # type: ignore
            try:
                _response_json = _response.json()
            except JSONDecodeError:
                raise ApiError(status_code=_response.status_code, body=_response.text)
            raise ApiError(status_code=_response.status_code, body=_response_json)

    def upload(
        self,
        corpus_key: CorpusKey,
        *,
        file: core.File,
        metadata: typing.Optional[typing.Dict[str, typing.Any]] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> Document:
        """
        Upload files such as PDFs and Word Documents. Vectara will attempt to automatically extract text and any metadata.

        Parameters
        ----------
        corpus_key : CorpusKey
            The unique key identifying the corpus of which to upload the file.

        file : core.File
            See core.File for more documentation

        metadata : typing.Optional[typing.Dict[str, typing.Any]]
            Arbitrary object that will be attached as document metadata to the extracted document.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        Document
            The extracted document havs been parsed and added to the corpus.

        Examples
        --------
        from vectara.client import Vectara

        client = Vectara(
            api_key="YOUR_API_KEY",
        )
        client.upload(
            corpus_key="my-corpus",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"v2/corpora/{jsonable_encoder(corpus_key)}/upload_file",
            method="POST",
            data={"metadata": metadata},
            files={"file": file},
            request_options=request_options,
            omit=OMIT,
        )
        if 200 <= _response.status_code < 300:
            return pydantic_v1.parse_obj_as(Document, _response.json())  # type: ignore
        if _response.status_code == 400:
            raise BadRequestError(pydantic_v1.parse_obj_as(BadRequestErrorBody, _response.json()))  # type: ignore
        if _response.status_code == 403:
            raise ForbiddenError(pydantic_v1.parse_obj_as(Error, _response.json()))  # type: ignore
        if _response.status_code == 404:
            raise NotFoundError(pydantic_v1.parse_obj_as(NotFoundErrorBody, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)


class AsyncVectara:
    """
    Use this class to access the different functions within the SDK. You can instantiate any number of clients with different configuration that will propagate to these functions.

    Parameters
    ----------
    base_url : typing.Optional[str]
        The base url to use for requests from the client.

    environment : VectaraEnvironment
        The environment to use for requests from the client. from .environment import VectaraEnvironment



        Defaults to VectaraEnvironment.DEFAULT



    api_key : typing.Optional[str]
    client_id : typing.Optional[str]
    client_secret : typing.Optional[str]
    _token_getter_override : typing.Optional[typing.Callable[[], str]]
    timeout : typing.Optional[float]
        The timeout to be used, in seconds, for requests. By default the timeout is 60 seconds, unless a custom httpx client is used, in which case this default is not enforced.

    follow_redirects : typing.Optional[bool]
        Whether the default httpx client follows redirects or not, this is irrelevant if a custom httpx client is passed in.

    httpx_client : typing.Optional[httpx.AsyncClient]
        The httpx client to use for making requests, a preconfigured client is used by default, however this is useful should you want to pass in any custom httpx configuration.

    Examples
    --------
    from vectara.client import AsyncVectara

    client = AsyncVectara(
        api_key="YOUR_API_KEY",
    )
    """

    def __init__(
        self,
        *,
        base_url: typing.Optional[str] = None,
        environment: VectaraEnvironment = VectaraEnvironment.DEFAULT,
        api_key: typing.Optional[str] = os.getenv("VECTARA_API_KEY"),
        client_id: typing.Optional[str] = os.getenv("VECTARA_CLIENT_ID"),
        client_secret: typing.Optional[str] = os.getenv("VECTARA_CLIENT_SECRET"),
        _token_getter_override: typing.Optional[typing.Callable[[], str]] = None,
        timeout: typing.Optional[float] = None,
        follow_redirects: typing.Optional[bool] = True,
        httpx_client: typing.Optional[httpx.AsyncClient] = None,
    ):
        _defaulted_timeout = timeout if timeout is not None else 60 if httpx_client is None else None
        if client_id is None:
            raise ApiError(
                body="The client must be instantiated be either passing in client_id or setting VECTARA_CLIENT_ID"
            )
        if client_secret is None:
            raise ApiError(
                body="The client must be instantiated be either passing in client_secret or setting VECTARA_CLIENT_SECRET"
            )
        oauth_token_provider = OAuthTokenProvider(
            client_id=client_id,
            client_secret=client_secret,
            client_wrapper=SyncClientWrapper(
                base_url=_get_base_url(base_url=base_url, environment=environment),
                api_key=api_key,
                httpx_client=httpx.Client(timeout=_defaulted_timeout, follow_redirects=follow_redirects)
                if follow_redirects is not None
                else httpx.Client(timeout=_defaulted_timeout),
                timeout=_defaulted_timeout,
            ),
        )
        self._client_wrapper = AsyncClientWrapper(
            base_url=_get_base_url(base_url=base_url, environment=environment),
            api_key=api_key,
            token=_token_getter_override if _token_getter_override is not None else oauth_token_provider.get_token,
            httpx_client=httpx_client
            if httpx_client is not None
            else httpx.AsyncClient(timeout=_defaulted_timeout, follow_redirects=follow_redirects)
            if follow_redirects is not None
            else httpx.AsyncClient(timeout=_defaulted_timeout),
            timeout=_defaulted_timeout,
        )
        self.api_keys = AsyncApiKeysClient(client_wrapper=self._client_wrapper)
        self.application_clients = AsyncApplicationClientsClient(client_wrapper=self._client_wrapper)
        self.auth = AsyncAuthClient(client_wrapper=self._client_wrapper)
        self.chats = AsyncChatsClient(client_wrapper=self._client_wrapper)
        self.corpus = AsyncCorpusClient(client_wrapper=self._client_wrapper)
        self.documents = AsyncDocumentsClient(client_wrapper=self._client_wrapper)
        self.encoders = AsyncEncodersClient(client_wrapper=self._client_wrapper)
        self.jobs = AsyncJobsClient(client_wrapper=self._client_wrapper)
        self.llms = AsyncLlmsClient(client_wrapper=self._client_wrapper)
        self.rerankers = AsyncRerankersClient(client_wrapper=self._client_wrapper)
        self.users = AsyncUsersClient(client_wrapper=self._client_wrapper)

    @typing.overload
    async def query(
        self,
        *,
        stream_response: typing.Literal[True],
        query: str,
        search: SearchCorporaParameters,
        generation: typing.Optional[GenerationParameters] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.AsyncIterator[QueryStreamedResponse]:
        """
        Perform a multi-purpose query that can retrieve relevant information from one or more corpora and generate a response using RAG.

        Parameters
        ----------
        stream_response: bool 
            Whether to stream the response

        query : str
            The query to receive an answer on.

        search : SearchCorporaParameters

        generation : typing.Optional[GenerationParameters]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Yields
        ------
        typing.AsyncIterator[QueryStreamedResponse]


        Examples
        --------
        from vectara import (
            CitationParameters,
            ContextConfiguration,
            GenerationParameters,
            KeyedSearchCorpus,
            ModelParameters,
            SearchCorporaParameters,
            SearchReranker_CustomerReranker,
        )
        from vectara.client import AsyncVectara

        client = AsyncVectara(
            api_key="YOUR_API_KEY",
        )
        response = await client.query(
            query="string",
            search=SearchCorporaParameters(
                corpora=[KeyedSearchCorpus()],
                offset=1,
                limit=1,
                context_configuration=ContextConfiguration(),
                reranker=SearchReranker_CustomerReranker(),
            ),
            generation=GenerationParameters(
                prompt_name="string",
                max_used_search_results=1,
                prompt_text="string",
                max_response_characters=1,
                response_language="auto",
                model_parameters=ModelParameters(
                    max_tokens=1,
                    temperature=1.1,
                    frequency_penalty=1.1,
                    presence_penalty=1.1,
                ),
                citations=CitationParameters(),
                enable_factual_consistency_score=True,
            ),
        )
        async for chunk in response:
            yield chunk
        """
        ...

    @typing.overload
    async def query(
        self,
        *,
        stream_response: typing.Literal[False] = False,
        query: str,
        search: SearchCorporaParameters,
        generation: typing.Optional[GenerationParameters] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> QueryFullResponse:
        """
        Perform a multi-purpose query that can retrieve relevant information from one or more corpora and generate a response using RAG.

        Parameters
        ----------
        stream_response: bool 
            Whether to stream the response

        query : str
            The query to receive an answer on.

        search : SearchCorporaParameters

        generation : typing.Optional[GenerationParameters]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        QueryFullResponse


        Examples
        --------
        from vectara import (
            CitationParameters,
            ContextConfiguration,
            GenerationParameters,
            KeyedSearchCorpus,
            ModelParameters,
            SearchCorporaParameters,
            SearchReranker_CustomerReranker,
        )
        from vectara.client import AsyncVectara

        client = AsyncVectara(
            api_key="YOUR_API_KEY",
        )
        await client.query(
            query="Am I allowed to bring pets to work?",
            search=SearchCorporaParameters(
                corpora=[
                    KeyedSearchCorpus(
                        custom_dimensions={},
                        metadata_filter='doc.title = "Adventures of Huckleberry Finn"',
                        lexical_interpolation=0.025,
                        semantics="default",
                        corpus_key="my-corpus",
                    )
                ],
                offset=0,
                limit=10,
                context_configuration=ContextConfiguration(
                    characters_before=30,
                    characters_after=30,
                    sentences_before=3,
                    sentences_after=3,
                    start_tag="<em>",
                    end_tag="</em>",
                ),
                reranker=SearchReranker_CustomerReranker(
                    reranker_id="rnk_272725719",
                ),
            ),
            generation=GenerationParameters(
                prompt_name="vectara-summary-ext-v1.2.0",
                max_used_search_results=5,
                prompt_text='[\n  {"role": "system", "content": "You are a helpful search assistant."},\n  #foreach ($qResult in $vectaraQueryResults)\n    {"role": "user", "content": "Given the $vectaraIdxWord[$foreach.index] search result."},\n    {"role": "assistant", "content": "${qResult.getText()}" },\n  #end\n  {"role": "user", "content": "Generate a summary for the query \'${vectaraQuery}\' based on the above results."}\n]\n',
                max_response_characters=300,
                response_language="auto",
                model_parameters=ModelParameters(
                    max_tokens=0,
                    temperature=0.0,
                    frequency_penalty=0.0,
                    presence_penalty=0.0,
                ),
                citations=CitationParameters(
                    style="none",
                    url_pattern="https://vectara.com/documents/{doc.id}",
                    text_pattern="{doc.title}",
                ),
                enable_factual_consistency_score=True,
            ),
        )
        """
        ...

    async def query(
        self,
        *,
        stream_response: bool = False,
        query: str,
        search: SearchCorporaParameters,
        generation: typing.Optional[GenerationParameters] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.Union[QueryFullResponse, typing.AsyncIterator[QueryStreamedResponse]]:
        """
        Perform a multi-purpose query that can retrieve relevant information from one or more corpora and generate a response using RAG.

        Parameters
        ----------
        stream_response: bool 
            Whether to stream the response

        query : str
            The query to receive an answer on.

        search : SearchCorporaParameters

        generation : typing.Optional[GenerationParameters]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.Union[QueryFullResponse, typing.AsyncIterator[QueryStreamedResponse]]


        Examples
        --------
        from vectara import (
            CitationParameters,
            ContextConfiguration,
            GenerationParameters,
            KeyedSearchCorpus,
            ModelParameters,
            SearchCorporaParameters,
            SearchReranker_CustomerReranker,
        )
        from vectara.client import AsyncVectara

        client = AsyncVectara(
            api_key="YOUR_API_KEY",
        )
        await client.query(
            query="Am I allowed to bring pets to work?",
            search=SearchCorporaParameters(
                corpora=[
                    KeyedSearchCorpus(
                        custom_dimensions={},
                        metadata_filter='doc.title = "Adventures of Huckleberry Finn"',
                        lexical_interpolation=0.025,
                        semantics="default",
                        corpus_key="my-corpus",
                    )
                ],
                offset=0,
                limit=10,
                context_configuration=ContextConfiguration(
                    characters_before=30,
                    characters_after=30,
                    sentences_before=3,
                    sentences_after=3,
                    start_tag="<em>",
                    end_tag="</em>",
                ),
                reranker=SearchReranker_CustomerReranker(
                    reranker_id="rnk_272725719",
                ),
            ),
            generation=GenerationParameters(
                prompt_name="vectara-summary-ext-v1.2.0",
                max_used_search_results=5,
                prompt_text='[\n  {"role": "system", "content": "You are a helpful search assistant."},\n  #foreach ($qResult in $vectaraQueryResults)\n    {"role": "user", "content": "Given the $vectaraIdxWord[$foreach.index] search result."},\n    {"role": "assistant", "content": "${qResult.getText()}" },\n  #end\n  {"role": "user", "content": "Generate a summary for the query \'${vectaraQuery}\' based on the above results."}\n]\n',
                max_response_characters=300,
                response_language="auto",
                model_parameters=ModelParameters(
                    max_tokens=0,
                    temperature=0.0,
                    frequency_penalty=0.0,
                    presence_penalty=0.0,
                ),
                citations=CitationParameters(
                    style="none",
                    url_pattern="https://vectara.com/documents/{doc.id}",
                    text_pattern="{doc.title}",
                ),
                enable_factual_consistency_score=True,
            ),
        )
        """
        if stream_response: 
            _response = await self._client_wrapper.httpx_client.request(
                "v2/query",
                method="POST",
                json={"query": query, "search": search, "generation": generation, "stream_response": False},
                request_options=request_options,
                omit=OMIT,
            )
            if 200 <= _response.status_code < 300:
                return pydantic_v1.parse_obj_as(QueryFullResponse, _response.json())  # type: ignore
            if _response.status_code == 400:
                raise BadRequestError(pydantic_v1.parse_obj_as(BadRequestErrorBody, _response.json()))  # type: ignore
            if _response.status_code == 403:
                raise ForbiddenError(pydantic_v1.parse_obj_as(Error, _response.json()))  # type: ignore
            if _response.status_code == 404:
                raise NotFoundError(pydantic_v1.parse_obj_as(NotFoundErrorBody, _response.json()))  # type: ignore
            try:
                _response_json = _response.json()
            except JSONDecodeError:
                raise ApiError(status_code=_response.status_code, body=_response.text)
            raise ApiError(status_code=_response.status_code, body=_response_json)
        else: 
            _response = await self._client_wrapper.httpx_client.request(
                "v2/query",
                method="POST",
                json={"query": query, "search": search, "generation": generation, "stream_response": False},
                request_options=request_options,
                omit=OMIT,
            )
            if 200 <= _response.status_code < 300:
                return pydantic_v1.parse_obj_as(QueryFullResponse, _response.json())  # type: ignore
            if _response.status_code == 400:
                raise BadRequestError(pydantic_v1.parse_obj_as(BadRequestErrorBody, _response.json()))  # type: ignore
            if _response.status_code == 403:
                raise ForbiddenError(pydantic_v1.parsxe_obj_as(Error, _response.json()))  # type: ignore
            if _response.status_code == 404:
                raise NotFoundError(pydantic_v1.parse_obj_as(NotFoundErrorBody, _response.json()))  # type: ignore
            try:
                _response_json = _response.json()
            except JSONDecodeError:
                raise ApiError(status_code=_response.status_code, body=_response.text)
            raise ApiError(status_code=_response.status_code, body=_response_json)

    async def upload(
        self,
        corpus_key: CorpusKey,
        *,
        file: core.File,
        metadata: typing.Optional[typing.Dict[str, typing.Any]] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> Document:
        """
        Upload files such as PDFs and Word Documents. Vectara will attempt to automatically extract text and any metadata.

        Parameters
        ----------
        corpus_key : CorpusKey
            The unique key identifying the corpus of which to upload the file.

        file : core.File
            See core.File for more documentation

        metadata : typing.Optional[typing.Dict[str, typing.Any]]
            Arbitrary object that will be attached as document metadata to the extracted document.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        Document
            The extracted document havs been parsed and added to the corpus.

        Examples
        --------
        from vectara.client import AsyncVectara

        client = AsyncVectara(
            api_key="YOUR_API_KEY",
        )
        await client.upload(
            corpus_key="my-corpus",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"v2/corpora/{jsonable_encoder(corpus_key)}/upload_file",
            method="POST",
            data={"metadata": metadata},
            files={"file": file},
            request_options=request_options,
            omit=OMIT,
        )
        if 200 <= _response.status_code < 300:
            return pydantic_v1.parse_obj_as(Document, _response.json())  # type: ignore
        if _response.status_code == 400:
            raise BadRequestError(pydantic_v1.parse_obj_as(BadRequestErrorBody, _response.json()))  # type: ignore
        if _response.status_code == 403:
            raise ForbiddenError(pydantic_v1.parse_obj_as(Error, _response.json()))  # type: ignore
        if _response.status_code == 404:
            raise NotFoundError(pydantic_v1.parse_obj_as(NotFoundErrorBody, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)


def _get_base_url(*, base_url: typing.Optional[str] = None, environment: VectaraEnvironment) -> str:
    if base_url is not None:
        return base_url
    elif environment is not None:
        return environment.value
    else:
        raise Exception("Please pass in either base_url or environment to construct the client")
